{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 - Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies\n",
    "Most importantly install [Unity ML-agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md), PyTorch, and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the Unity enviroment downloaded and change the path of the file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exame the State and Action Spaces\n",
    "The simulation contains a single agent that navigates a large environment. At each time step, it has four actions at its disposal:\n",
    "\n",
    "- 0 - walk forward\n",
    "- 1 - walk backward\n",
    "- 2 - turn left\n",
    "- 3 - turn right\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction. A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the enviroment\n",
    "print(\"Number of agents:\", len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print(\"Number of actions:\", action_size)\n",
    "\n",
    "# examine the state space\n",
    "state = env_info.vector_observations[0]\n",
    "print(\"Example of a state:\", state)\n",
    "state_size = len(state)\n",
    "print(\"States have length of:\", state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate and initialize the agent\n",
    "The learning agent is imported from a separate file \"./agent.py\" and takes `state_size`, `action_size` and a `seed` as instance variables.\n",
    "\n",
    "A few highlights of the agent:\n",
    "- The agent follows an epsilon-greedy policy \n",
    "- The agent uses a buffer to store recent steps `(state, action, reward, next_state, done)` tuples and replay them\n",
    "- The agent maximizes reward based on a deep Q-learning network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the untrained agent\n",
    "Run an **untrained** agent for 200 time steps to see what happens to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]      # reset environment\n",
    "state = env_info.vector_observations[0]                 # get first state from the reseted environment\n",
    "score = 0\n",
    "for j in range(200):\n",
    "    action = agent.act(state)                           # agent select an action based on policy and current state\n",
    "    env_info = env.step(action)[brain_name]             # send the action to the enviroment\n",
    "    next_state = env_info.vector_observations[0]        # get the next state\n",
    "    reward = env_info.rewards[0]                        # get the reward\n",
    "    done = env_info.local_done[0]                       # check if the episode has finished\n",
    "    score += reward                                     # update the total score\n",
    "    state = next_state                                  # set the state as the next state for the following step\n",
    "    if done:                                            # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train an agent with Deep Q-Network (DQN)\n",
    "The agent actually runs on an underlying Q-learning network for large state spaces (even though the enviroment's state space is discrete at 37, it is too large to populate and to calculate a Q-Table at every step. Therefore, we make use of a Q-learning network and enhance this with multiple layers, hence Deep Q-learning Network (or DQN, for short).\n",
    "\n",
    "Let's train the agent until it achieves a average score of +13 over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    '''\n",
    "    -------------------------------------------\n",
    "    Parameters\n",
    "    \n",
    "    n_episodes: # of episodes that the agent is training for\n",
    "    max_t:      # of time steps (max) the agent is taking per episode\n",
    "    eps_start:  start value of epsilon for the epsilon-greedy policy\n",
    "    eps_end:    terminal value of epsilon\n",
    "    eps_decay:  discount rate of epsilon for each episode\n",
    "    -------------------------------------------\n",
    "    '''\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]       # turn on train mode of the environment\n",
    "        state = env_info.vector_observations[0]                 # select first state\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                      # agent select an action based on policy and current state\n",
    "            env_info = env.step(action)[brain_name]             # send action to the environment\n",
    "            next_state = env_info.vector_observations[0]        # get next state from the enviroment\n",
    "            reward = env_info.rewards[0]                        # get reward\n",
    "            done = env_info.local_done[0]                       # check if the episode has finished\n",
    "            agent.step(state, action, reward, next_state, done) # agent records enviroment response in recent step\n",
    "            state = next_state                                  # set the state as the next state for the following step\n",
    "            score += reward                                     # update the total score\n",
    "            if done:                                            # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)                           \n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        # print average 100-episode score for each episode\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        # print average 100-episode score\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # print and save Q-Network weights when a score of +13 over 100 episodes has been achieved \n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnviroment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_wandb.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the scores\n",
    "Plot the scores according to their episodes. We can see a gradual increase in the scores as we increase the training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x = np.arange(len(scores))\n",
    "y = scores\n",
    "\n",
    "# plot scores\n",
    "plt.plot(x, y)\n",
    "# plot trendline\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x,p(x),\"r-\", linewidth=5)\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test a trained agent\n",
    "Run a **trained** agent for 200 time steps to see what happens to the score. Compare this with the score of the untrained agent from 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    agent.qnetwork_local.load_state_dict(checkpoint)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = trained_agent(\"checkpoint_dqn.pth\")\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]      # reset environment\n",
    "state = env_info.vector_observations[0]                 # get first state from the reseted environment\n",
    "score = 0\n",
    "for j in range(200):\n",
    "    action = agent.act(state)                           # agent select an action based on policy and current state\n",
    "    env_info = env.step(action)[brain_name]             # send the action to the enviroment\n",
    "    next_state = env_info.vector_observations[0]        # get the next state\n",
    "    reward = env_info.rewards[0]                        # get the reward\n",
    "    done = env_info.local_done[0]                       # check if the episode has finished\n",
    "    score += reward                                     # update the total score\n",
    "    state = next_state                                  # set the state as the next state for the following step\n",
    "    if done:                                            # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
